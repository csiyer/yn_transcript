{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trial transcript yes/no analysis\n",
    "\n",
    "This code will read trial transcript PDFs and for each witness (and each questioner) quantify how many yes/no questions that witness is asked.\n",
    "\n",
    "Authors: Chris Iyer, Miles Zoltak\n",
    "Updated: 5/20/2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input:\n",
    "- file path of folder containing transcript PDFs\n",
    "\n",
    "Desired output:\n",
    "- quantified # of yes/no questions + total questions asked to each witness, by each questioner\n",
    "\n",
    "Pipeline:\n",
    "- Extract text from PDF with OCR\n",
    "- Identify start of each witness's questioning\n",
    "- Identify questions, and who is asking them\n",
    "- Classify questions as yes/no or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading and processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "dir_path = \"example_transcripts\"\n",
    "files = [f for f in sorted(os.listdir(dir_path))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pypdf\n",
      "  Downloading pypdf-4.2.0-py3-none-any.whl (290 kB)\n",
      "     ---------------------------------------- 0.0/290.4 kB ? eta -:--:--\n",
      "     -------------------------------------- 290.4/290.4 kB 9.0 MB/s eta 0:00:00\n",
      "Installing collected packages: pypdf\n",
      "Successfully installed pypdf-4.2.0\n",
      "Requirement already satisfied: tqdm in c:\\users\\ciyer\\miniconda3\\envs\\lab\\lib\\site-packages (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\ciyer\\miniconda3\\envs\\lab\\lib\\site-packages (from tqdm) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install pypdf\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing PDFs to text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:33<00:00,  1.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from pypdf import PdfReader\n",
    "from tqdm import tqdm\n",
    "\n",
    "reader_dict = {fname: PdfReader(os.path.join(dir_path, fname)) for fname in files}\n",
    "\n",
    "print('Processing PDFs to text...')\n",
    "# pages_dict = {fname: [page.extract_text() for page in reader.pages] for fname, reader in reader_dict.items()}\n",
    "pages_dict = {}\n",
    "for fname, reader in tqdm(reader_dict.items(), total=len(reader_dict)):\n",
    "  pages_dict[fname] = [page.extract_text() for page in reader.pages]\n",
    "print('finished!\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second bullet point idk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "first_real_page = pages1[2]\n",
    "\n",
    "# print get rid of watermark\n",
    "watermark_regex = r\"^\\s*ROUGH DRAFT\"\n",
    "first_real_page = re.sub(watermark_regex, '', first_real_page)\n",
    "\n",
    "\n",
    "# for each line, remove a leading line number\n",
    "line_numbers_regex = r\"^\\d+$\"\n",
    "lines = first_real_page.split(\"\\n\")\n",
    "processed_lines = []\n",
    "for line in lines:\n",
    "  processed_lines.append(re.sub(line_numbers_regex, \"\", line))\n",
    "\n",
    "first_real_page = \"\\n\".join(processed_lines)\n",
    "\n",
    "# strip off any leading or trailing whitespace\n",
    "first_real_page = first_real_page.strip()\n",
    "print(first_real_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_watermark(page):\n",
    "  watermark_regex = r\"^\\s*ROUGH DRAFT\"\n",
    "  page = re.sub(watermark_regex, '', page)\n",
    "  return page\n",
    "\n",
    "def remove_line_numbers(page):\n",
    "  line_numbers_regex = r\"^\\d+$\"\n",
    "  lines = page.split(\"\\n\")\n",
    "  processed_lines = []\n",
    "  for line in lines:\n",
    "    processed_lines.append(re.sub(line_numbers_regex, \"\", line))\n",
    "\n",
    "  page = \"\\n\".join(processed_lines)\n",
    "  return page\n",
    "\n",
    "def process_page(page):\n",
    "  # remove watermarks and line numbers\n",
    "  page = remove_watermark(page)\n",
    "  page = remove_line_numbers(page)\n",
    "\n",
    "  # strip off any leading or trailing whitespace\n",
    "  page = page.strip()\n",
    "\n",
    "  return page\n",
    "\n",
    "def process_transcript(transcript_pages):\n",
    "  # drop the first two pages\n",
    "  transcript_pages = transcript_pages[2:]\n",
    "\n",
    "  # process each page\n",
    "  processed_pages = [process_page(page) for page in transcript_pages]\n",
    "\n",
    "  return processed_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = process_transcript(pages1)\n",
    "t2 = process_transcript(pages2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_name(line):\n",
    "  colon_idx = line.find(\":\")\n",
    "  if colon_idx == -1:\n",
    "    return None\n",
    "\n",
    "  name = line[:colon_idx].strip()\n",
    "  return name if name == name.upper() else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = False\n",
    "for i, page in enumerate(t1):\n",
    "  lines = page.split(\"\\n\")\n",
    "  for j, line in enumerate(lines):\n",
    "    name = extract_name(line)\n",
    "    # if name: print(name\n",
    "    if name == \"BY MS. FOG\":\n",
    "      print(t1[i])\n",
    "      print(i)\n",
    "      stop = True\n",
    "      break\n",
    "  if stop: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf_path_clean = \"drive/My Drive/pdf_stuff/pulp_fiction_cleaner.pdf\"\n",
    "pf_reader_clean = PdfReader(pf_path_clean)\n",
    "pf_pages_clean = [page.extract_text().lower() for page in pf_reader_clean.pages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"shahrukhx01/question-vs-statement-classifier\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"shahrukhx01/question-vs-statement-classifier\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"shahrukhx01/question-vs-statement-classifier\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"shahrukhx01/question-vs-statement-classifier\")\n",
    "\n",
    "def classify_text(text):\n",
    "    # Tokenize the text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "    # Perform inference\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    # Apply softmax to logits\n",
    "    probabilities = F.softmax(outputs.logits, dim=1)\n",
    "\n",
    "    # Get probability of being a question\n",
    "    probability_question = probabilities[0][1].item()  # Probability for 'question' class\n",
    "\n",
    "    return probability_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = []\n",
    "for page in pf_pages_clean[1:]:\n",
    "  lines = page.split(\"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
