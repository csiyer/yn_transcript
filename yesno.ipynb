{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trial transcript yes/no analysis\n",
    "\n",
    "This code will read trial transcript PDFs and for each witness (and each questioner) quantify how many yes/no questions that witness is asked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input:\n",
    "- file path of folder containing transcript PDFs (currently, this should be run separately for each case/trial)\n",
    "\n",
    "Output:\n",
    "- writes a text file containing witness statistics, for each examiner, of # of yes/no questions and # total questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: there are some instances where the PDF reader just misses some lines, so this won't be 100% accurate. This code contains a couple shortcuts for guessing information that was lost in the PDF reading process.\n",
    "\n",
    "For example, look at `12RT.pdf` page 95 // loc 1721. Compare to `entire_transcript[1984825: 1984890]` or `lines[77314:77316]`--these are missing two lines between \"DIRECT EXAMINATION\" and \"A. POLICE OFFICER WITH THE...\"\n",
    "\n",
    "In this example, the examiner is not identified, and the question asked is not identified. This is rare. But in these rare cases, we will guess who the examiner is, and try to infer whether it was a yes/no question from the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ciyer\\miniconda3\\envs\\hcrc\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# %pip install -r requirements.txt\n",
    "\n",
    "import os, re, argparse\n",
    "from datetime import datetime\n",
    "from pypdf import PdfReader\n",
    "from tqdm.auto import tqdm\n",
    "from collections import defaultdict\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################### DATA LOADING AND PROCESSING ###############################\n",
    "\n",
    "def get_lines(INPUT_DIRECTORY_PATH):\n",
    "    \n",
    "    files = [f for f in sorted(os.listdir(INPUT_DIRECTORY_PATH)) if f.endswith('.pdf')]\n",
    "\n",
    "    # Read all the PDFs into a huge string, and then split into a big list of lines\n",
    "    entire_transcript = \"\"\n",
    "    for file in tqdm(files, total=len(files), desc=\"Processing PDFs to text...\"):\n",
    "        reader = PdfReader(os.path.join(INPUT_DIRECTORY_PATH, file))\n",
    "        for page in reader.pages:\n",
    "            entire_transcript += page.extract_text() + '\\n'\n",
    "\n",
    "    # Separate into lines, and filter out the ones that are just line numbers, e.g. \"24 \"\n",
    "    lines = entire_transcript.split('\\n')\n",
    "    lines = [line for line in lines if not re.match(r'^[\\d\\s]*$', line)]\n",
    "\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################### LOAD QUESTION CLASSIFIER ###################################\n",
    "\n",
    "classifier = None\n",
    "def get_classifier():\n",
    "    # load question classification model from local. Or, if local doesn't exist, download from HuggingFace and save to local\n",
    "    local_model_path = './model_local'\n",
    "    model_name = 'PrimeQA/tydi-boolean_question_classifier-xlmr_large-20221117'\n",
    "\n",
    "    try: \n",
    "        # try to load local model\n",
    "        tokenizer = AutoTokenizer.from_pretrained(local_model_path)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(local_model_path)\n",
    "        print(f\"Loaded model from {local_model_path}\")\n",
    "    except: \n",
    "        # If loading locally fails, download and save the model\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "        # Save the model locally\n",
    "        tokenizer.save_pretrained(local_model_path)\n",
    "        model.save_pretrained(local_model_path)\n",
    "        print(f\"Downloaded and saved model to {local_model_path}\")\n",
    "\n",
    "    global classifier\n",
    "    classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################### ANALYSIS HELPER FUNCTIONS  ################################\n",
    "\n",
    "def remove_whitespace(text):\n",
    "    return re.sub(r'[\\t\\n]', ' ', text)\n",
    "\n",
    "def only_letters_numbers_normal_punctuation(text):\n",
    "    out = re.sub(r'[^a-zA-Z0-9().,?!\\-\"\\':;/ ]', '', text)\n",
    "    if out.startswith('.'): out = out[1:].strip()\n",
    "    return out\n",
    "\n",
    "def clean_simple_line(line):\n",
    "    # removes punctuation/numbers/non-letters\n",
    "    return re.sub(r'[^a-zA-Z\\s]', '', line).upper().strip() \n",
    "\n",
    "def clean_question(question):\n",
    "    clean =  re.sub(r'[\\t\\n\\s+]', ' ', question) # tabs, newlines, and extra spaces\n",
    "    clean = re.sub(r'^\\d+\\s*', '', clean) # leading number and whitespace (line number)\n",
    "    clean = re.sub(r'[\"]|Q |Q. |Q . |Q• |Q • |Q- |', '', clean)  # Question marker\n",
    "    return clean.upper().strip()\n",
    "\n",
    "def line_is_witness_identifier(lines, i):\n",
    "    line = clean_simple_line(lines[i])\n",
    "    words = line.split(' ')\n",
    "    return len(words) < 6 and i < len(lines)-1 and ' as a witness' in lines[i+1].lower()\n",
    "\n",
    "def who_presents_this_witness(lines, witness_line_i): \n",
    "    for j in range(witness_line_i+1, witness_line_i+5): # scan the next few lines for keywords\n",
    "        if 'people' in lines[j].lower():\n",
    "            return 'people'\n",
    "        if 'defense' in lines[j].lower() or 'defendant' in lines[j].lower():\n",
    "            return 'defense'\n",
    "    return 'unknown'\n",
    "\n",
    "def line_is_examiner_identifier(line):\n",
    "    # each examination begins with a line like \"By Mr. Smith:\"  \n",
    "    line = re.sub(r'\\d+', '', line).strip() # eliminate leading numbers + whitespace. we don't want clean_simple_line because we want to keep colon if there is one\n",
    "    return len(line.split(' ')) < 6 and line[0:2].lower() == 'by' and line.strip()[-1] == ':'\n",
    "\n",
    "def clean_examiner_name(examiner_line):\n",
    "    if '.' in examiner_line and ':' in examiner_line:\n",
    "        name_substr = examiner_line[examiner_line.find('.'):examiner_line.find(':')]\n",
    "        return clean_simple_line(name_substr)\n",
    "    \n",
    "    name_followed_by_colon = [w for w in examiner_line.split(' ') if ':' in w][0]\n",
    "    return clean_simple_line(name_followed_by_colon)\n",
    "\n",
    "def line_is_examination_identifier(lines, i):\n",
    "    line = clean_simple_line(lines[i])\n",
    "    return len(line.split()) < 4 and 'EXAMINATION' in line and ('CROSS' in line or 'DIRECT' in line) and i < len(lines)-1 and ( \n",
    "        line_is_examiner_identifier(lines[i+1]) or lines[i+1].startswith('Q.') or lines[i+1].startswith('A.')\n",
    "        )\n",
    "\n",
    "def is_answer(line):\n",
    "    starts_a = re.sub(r'[^a-zA-Z. ]', '', line).strip().startswith('A. ') or re.sub(r' ', '', line).strip().startswith('.A.')\n",
    "    not_time = not re.sub(r'[^a-zA-Z\\.]', '', line).startswith('A.M.')\n",
    "    return starts_a and not_time\n",
    "    # return re.sub(r'[^a-zA-Z. ]', '', line).strip().startswith('A. ') and not re.sub(r'[^a-zA-Z\\.]', '', line).startswith('A.M.') # or line.strip().startswith('THE WITNESS:')\n",
    "\n",
    "def starts_question(text, current_examiner):\n",
    "    return any(item in text for item in ['Q. ', 'Q . ', 'Q• ', 'Q • ', 'Q- ', current_examiner+':']) # and '?' in text \n",
    "\n",
    "def get_previous_question(lines, i, current_examiner):\n",
    "    # if the previous question was not read in properly with 'Q.', then we want to parse what the question was when we hit an answer\n",
    "    possible_question = lines[i-1]\n",
    "    for j,prevline in enumerate(reversed(lines[i-11:i-1])): # check previous 10 lines for question, stop when we hit punctuation\n",
    "        prevline = remove_whitespace(prevline)\n",
    "        if starts_question(possible_question, current_examiner):\n",
    "            break\n",
    "        if prevline.strip().endswith(('.','!','?', ')')) or is_answer(prevline) or line_is_witness_identifier(lines, i-2-j) or line_is_examiner_identifier(prevline) or line_is_examination_identifier(lines, i-2-j):\n",
    "            break\n",
    "        possible_question = prevline + possible_question\n",
    "    return only_letters_numbers_normal_punctuation(possible_question)\n",
    "\n",
    "def is_yes_no_answer(lines, i, current_examiner):\n",
    "    # querying the model is more time-consuming, so we only want to do it if we cannot tell from the answer itself\n",
    "    answer = lines[i]\n",
    "    for nextline in lines[i+1:i+10]: # check next lines and add continuance of answer if necessary\n",
    "        if nextline.strip().endswith(('.','!','?')) or is_answer(nextline) or starts_question(nextline, current_examiner):\n",
    "            break\n",
    "        answer += nextline\n",
    "    answer_split = re.sub(r'[^A-Za-z ]', '', answer).upper().strip().split(' ')\n",
    "    if any(item in answer_split for item in ['YES', 'YEAH', 'YEP', 'NO', 'NOPE', 'UHHUH', 'UHUH', 'UMHUM', 'UMUM']) or 'NOT' in answer_split[0:3]:\n",
    "        if len(answer_split) < 8:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def is_yes_no(question):\n",
    "    # queries question classification model whether the question is a yes/no question or not, returns boolean\n",
    "    result = classifier(question)[0]['label']\n",
    "    if not result in ['LABEL_0', 'LABEL_1']:\n",
    "        return 'ERROR: unexpected classification result'\n",
    "    return result == 'LABEL_0' # model returns 'LABEL_0' for yes/no questions and 'LABEL_1' for other questions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFAULT EXAMINER GUESSES\n",
    "# there are some instances where the 'examiner identification' line isn't read properly by the pdf reader\n",
    "# for these, we need a default guess for who the examiner is.\n",
    "# so, we'll find the first direct examination for each side (people/defense) and save who the examiner is -- this is a good guess\n",
    "\n",
    "def get_default_examiners(lines):\n",
    "    DEFAULT_EXAMINER_KEY = {'people': '', 'defense': ''}\n",
    "    found = {'people': False, 'defense': False}\n",
    "    for i in range(len(lines)):\n",
    "        if line_is_witness_identifier(lines, i):\n",
    "            side = who_presents_this_witness(lines, i)\n",
    "            if side != 'unknown' and not found[side]:\n",
    "                # search the next 200 lines for a direct exam, if found one then get the examiner ID\n",
    "                direct_exam_found = True\n",
    "                for j,line in enumerate(lines[i:i+200]):\n",
    "                    if line_is_examination_identifier(lines, i+j) and 'DIRECT' in line:\n",
    "                        direct_exam_found = True\n",
    "                    if direct_exam_found and line_is_examiner_identifier(line):\n",
    "                        DEFAULT_EXAMINER_KEY[side] = clean_examiner_name(line)\n",
    "                        found[side] = True\n",
    "                        break\n",
    "        if found['people'] and found['defense']: \n",
    "            break\n",
    "        \n",
    "    print('Default examiner default guesses: ', DEFAULT_EXAMINER_KEY, '\\nIf these look incorrect, please stop and revise.')\n",
    "    return DEFAULT_EXAMINER_KEY\n",
    "\n",
    "def guess_examiner(witness_side, current_examination, DEFAULT_EXAMINER_KEY):\n",
    "    print('Examiner not found, guessing from previous records (this message should be rare).')\n",
    "    if 'DIRECT' in current_examination.upper():\n",
    "        return DEFAULT_EXAMINER_KEY[witness_side]\n",
    "    elif 'CROSS' in current_examination.upper():\n",
    "        other_side = [i for i in DEFAULT_EXAMINER_KEY.keys() if i != witness_side][0]\n",
    "        return DEFAULT_EXAMINER_KEY[other_side]\n",
    "    return 'error: unknown examiner'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################  TRANSCRIPT ANALYSIS  #####################################\n",
    "\n",
    "# loop through transcript to identify questions, and save the ones we need to classify as yes/no questions or not\n",
    "def analyze_transcript(lines, classifier, DEFAULT_EXAMINER_KEY):\n",
    "\n",
    "    current_witness = ''\n",
    "    current_witness_side = ''\n",
    "    current_examination = ''\n",
    "    current_examiner = ''\n",
    "    name_to_stats = defaultdict(lambda: defaultdict(lambda: {'total_questions': 0, 'yes_no_questions': 0})) # use default dict so we don't have to check if key already exists\n",
    "    questions_to_query = [] # to parallelize later\n",
    "\n",
    "    for i,line in enumerate(lines):\n",
    "\n",
    "        if line_is_witness_identifier(lines, i):\n",
    "            current_witness = clean_simple_line(line)\n",
    "            current_witness_side = who_presents_this_witness(lines, i)\n",
    "\n",
    "        elif line_is_examination_identifier(lines, i):\n",
    "            current_examiner = ''\n",
    "            current_examination = clean_simple_line(line)\n",
    "\n",
    "        elif line_is_examiner_identifier(line):\n",
    "            current_examiner = clean_examiner_name(line)\n",
    "\n",
    "        elif is_answer(line):\n",
    "\n",
    "            if current_examiner == '': # we may have missed this before, and have to guess now\n",
    "                current_examiner = guess_examiner(current_witness_side, current_examination, DEFAULT_EXAMINER_KEY) \n",
    "\n",
    "            question = get_previous_question(lines, i, current_examiner) \n",
    "                \n",
    "            if '?' in question: # to rule out things like \"Q. Good morning.\"\n",
    "                name_to_stats[current_witness][current_examiner]['total_questions'] += 1\n",
    "                \n",
    "                if is_yes_no_answer(lines, i, current_examiner): # this function catches answers that are easy to see are yes/no answers, so we don't have to waste time querying the model\n",
    "                    name_to_stats[current_witness][current_examiner]['yes_no_questions'] += 1\n",
    "                else:\n",
    "                    # not able to identify it as yes/no, add this question (and identifying information) to the pile of questions to query later\n",
    "                    questions_to_query.append((clean_question(question), current_witness, current_examiner))\n",
    "\n",
    "    return questions_to_query\n",
    "    print(f'Finished reading transcript, querying model with questions.')\n",
    "    # execute question classification (in parallel)\n",
    "    # try:\n",
    "    #     with ProcessPoolExecutor() as executor: \n",
    "    #         classifier_results = list(executor.map(is_yes_no, [q for q,_,_ in questions_to_query]))\n",
    "    # except:\n",
    "    #     print('Error in process pool, defaulting to threads')\n",
    "    #     with ThreadPoolExecutor() as executor:\n",
    "    #         classifier_results = list(executor.map(is_yes_no, [q for q,_,_ in questions_to_query]))\n",
    "\n",
    "    # add the results of these queries to our stats\n",
    "    for (_,witness,examiner),result in zip(questions_to_query, classifier_results):\n",
    "        name_to_stats[witness][examiner]['yes_no_questions'] += result\n",
    "\n",
    "    print(f'Finished analyzing transcript, saving output.')\n",
    "    return name_to_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################### OUTPUT TXT FILE ###########################################\n",
    "\n",
    "def write_output(name_to_stats, INPUT_DIRECTORY_PATH):\n",
    "    output_csv_text = 'Witness,Examiner,Total questions,Yes/No Questions,Yes/No Percentage\\n'\n",
    "\n",
    "    for name,values in name_to_stats.items():\n",
    "        for examiner, stats in values.items():\n",
    "            output_csv_text += f'{name},{examiner},{stats[\"yes_no_questions\"]},{stats[\"total_questions\"]},'\n",
    "            try:\n",
    "                percentage = round(stats['yes_no_questions'] / stats['total_questions'] * 100, 2)\n",
    "            except:\n",
    "                percentage = 'N/A'\n",
    "            output_csv_text += f'{percentage}\\n'\n",
    "        output_csv_text += '\\n'\n",
    "\n",
    "    def get_unique_id(lines):\n",
    "        for l in lines[0:30]:\n",
    "            if 'NO. ' in l: # case number\n",
    "                return 'case-' + l.split('NO. ')[1].strip()\n",
    "        return datetime.now().strftime('date-%Y-%m-%d_%H-%M')\n",
    "\n",
    "    output_path = os.path.join(INPUT_DIRECTORY_PATH, f'yesno_analysis_{get_unique_id(lines)}.csv')\n",
    "\n",
    "    with open(output_path, 'w') as file:\n",
    "        file.write(output_csv_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()\n",
    "        \n",
    "INPUT_DIRECTORY_PATH = parse_input_path()\n",
    "lines = get_lines(INPUT_DIRECTORY_PATH)\n",
    "init_classifier()\n",
    "DEFAULT_EXAMINER_KEY = get_default_examiners(lines)\n",
    "\n",
    "name_to_stats = analyze_transcript(lines, DEFAULT_EXAMINER_KEY)\n",
    "write_output(name_to_stats, INPUT_DIRECTORY_PATH)\n",
    "\n",
    "end_time = datetime.now()\n",
    "elapsed_minutes = (end_time - start_time).total_seconds() / 60\n",
    "print(f\"Script took {elapsed_minutes:.2f} minutes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
