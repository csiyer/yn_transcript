{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%conda activate lab # using a different environment with more of the necessary libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create labeled question set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SAVE ALL QUESTIONS TO CSV\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# split this process into ranges of lines corresponding to each witness (to parallelize for speed)\n",
    "# def process_one_range(range_of_lines, lines):\n",
    "# initialize variables to be used as we loop\n",
    "current_witness = ''\n",
    "current_witness_side = ''\n",
    "current_examination = ''\n",
    "current_examiner = ''\n",
    "qdata = pd.DataFrame(columns=['row_i', 'question_text', 'answer_yes_no', 'gpt_yes_no'])\n",
    "\n",
    "queries_to_make = []\n",
    "\n",
    "for i in range(len(lines)):\n",
    "    line = lines[i]\n",
    "\n",
    "    if line_is_witness_identifier(lines, i):\n",
    "        current_witness = clean_simple_line(line)\n",
    "        current_witness_side = who_presents_this_witness(lines, i)\n",
    "\n",
    "    elif line_is_examination_identifier(lines, i):\n",
    "        current_examiner = ''\n",
    "        current_examination = clean_simple_line(line)\n",
    "\n",
    "    elif line_is_examiner_identifier(line):\n",
    "        current_examiner = clean_examiner_name(line)\n",
    "\n",
    "    elif is_answer(line):\n",
    "        # we may need to guess necessary preceding info if there was an error in pdf reading\n",
    "        if current_examiner == '': \n",
    "            current_examiner = guess_examiner(current_witness_side, current_examination) \n",
    "\n",
    "        active_question = guess_previous_question(lines, i, current_examiner) \n",
    "            \n",
    "        if '?' in active_question: # to rule out things like \"Q. Good morning.\"\n",
    "            yes_no_answer = is_yes_no_answer(lines, i, current_examiner)\n",
    "            # yes_no_question = True if yes_no_answer=='yes' else is_yes_no(clean_question(active_question))\n",
    "            if yes_no_answer!='yes': queries_to_make.append((i, clean_question(active_question)))\n",
    "\n",
    "            yes_no_answer_tag = 'Y' if yes_no_answer=='yes' else 'n'\n",
    "            yes_no_question_tag = '' # 'Y' if yes_no_question else 'n'\n",
    "\n",
    "            qdata.loc[len(qdata)] = [i, clean_question(active_question), yes_no_answer_tag, yes_no_question_tag]\n",
    "    elif active_question:\n",
    "        active_question += line # if we started a question, add this line. resets at every answer or special identifying line\n",
    "\n",
    "print('queries to make: ', len(queries_to_make))\n",
    "qdata.to_csv('question_datasets/all_questions_unlabeled.csv', index=False)\n",
    "\n",
    "# def run(arg):\n",
    "#     i, question = arg\n",
    "#     return i, is_yes_no(question)\n",
    "# with ThreadPoolExecutor(max_workers=128) as executor:\n",
    "#     results = list(executor.map(run, queries_to_make))\n",
    "\n",
    "# for i,gpt_out in results:\n",
    "#     out = 'Y' if gpt_out else 'n'\n",
    "#     qdata.loc[i, 'gpt_yes_no'] = out\n",
    "# qdata.to_csv('all_questions_unlabeled.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence embedding\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "df = pd.read_csv('/Users/ciyer/Documents/yn_transcript/all_questions_unlabeled.csv')\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(df['question_text'].tolist())\n",
    "normalized_embeddings = normalize(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "pca = PCA()\n",
    "pca_result = pca.fit_transform(embeddings)\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "# Plot the explained variance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio.cumsum(), 'bo-')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance Ratio')\n",
    "plt.title('Explained Variance by Principal Components')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50-means clustering and save output\n",
    "n_clusters = 50 # elbow-ish of the PCA curve\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "cluster_labels = kmeans.fit_predict(normalized_embeddings)\n",
    "\n",
    "df['cluster'] = cluster_labels\n",
    "\n",
    "# Function to sample from each cluster\n",
    "def sample_from_cluster(cluster_id, n_samples=20):\n",
    "    cluster_indices = df[df['cluster'] == cluster_id].index\n",
    "    if len(cluster_indices) <= n_samples:\n",
    "        return cluster_indices\n",
    "    else:\n",
    "        return np.random.choice(cluster_indices, n_samples, replace=False)\n",
    "\n",
    "# Sample 20 sentences from each cluster\n",
    "sampled_indices = []\n",
    "for i in range(n_clusters):\n",
    "    sampled_indices.extend(sample_from_cluster(i))\n",
    "\n",
    "# Convert to list if it's not already\n",
    "sampled_indices = list(sampled_indices)\n",
    "\n",
    "# Shuffle the sampled indices\n",
    "np.random.shuffle(sampled_indices)\n",
    "\n",
    "# Ensure we have exactly 1000 samples\n",
    "if len(sampled_indices) > 1000:\n",
    "    sampled_indices = sampled_indices[:1000]\n",
    "\n",
    "df.loc[sampled_indices,].drop('cluster', axis=1).to_csv('/Users/ciyer/Documents/yn_transcript/training_questions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('/Users/ciyer/Documents/yn_transcript/question_datasets/labeled_training_questions.csv')\n",
    "train_data = df[df.answer_yes_no == 'no']\n",
    "\n",
    "models = {\n",
    "    'gpt': {'pipeline': ''},\n",
    "    \"facebook/bart-large-mnli\": {'yes/no question': 'yes', 'other question': 'no', 'pipeline': 'zero-shot-classification'},\n",
    "    \"scott-routledge/bert-question-classifier\": {'LABEL_0': 'yes', 'LABEL_1': 'no', 'LABEL_2': 'yes', 'pipeline': 'text-classification'},\n",
    "    \"sophiaqho/question_classifier_model_v2\": {'LABEL_0': 'yes', 'LABEL_1': 'no', 'pipeline': 'text-classification'},\n",
    "    \"alangpp255/Question_classifier_V2\": {'TF': 'yes', 'WH': 'no', 'pipeline': 'text-classification'},\n",
    "    \"ndavid/binary-question-classifier-bert\": {'LABEL_0': 'no', 'LABEL_1': 'yes', 'pipeline': 'text-classification'},\n",
    "    \"PrimeQA/tydi-boolean_question_classifier-xlmr_large-20221117\": {'LABEL_0': 'yes', 'LABEL_1': 'no', 'pipeline': 'text-classification'}\n",
    "}\n",
    "\n",
    "def add_model_preds(model_name):\n",
    "    pipe = models[model_name]['pipeline']\n",
    "    classifier = pipeline(pipe, model=model_name)\n",
    "    if pipe == 'zero-shot-classification':\n",
    "        candidate_labels = [\"yes/no question\", \"other question\"]\n",
    "        results = classifier(train_data['question_text'].tolist(), candidate_labels)\n",
    "        results = [r['labels'][0] for r in results]\n",
    "    elif pipe == 'text-classification':\n",
    "        results = [classifier(q)[0]['label'] for q in train_data.question_text]\n",
    "\n",
    "    predicted_labels = [models[model_name][p] for p in results]\n",
    "    train_data[model_name] = predicted_labels\n",
    "\n",
    "def evaluate_model(model_name):\n",
    "    print('Starting model: ', model_name)\n",
    "    \n",
    "    if model_name == 'gpt': model_name = 'gpt_yes_no'\n",
    "    if model_name not in train_data.columns:\n",
    "        add_model_preds(model_name)\n",
    "\n",
    "    accuracy = accuracy_score(train_data['manual_yes_no'], train_data[model_name])\n",
    "    cm = confusion_matrix(train_data['manual_yes_no'], train_data[model_name], labels=['no','yes'])\n",
    "\n",
    "    print(f\"Model: {model_name}, Accuracy: {accuracy:.4f}\")\n",
    "    plt.figure(figsize=(4,3))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['no','yes'], yticklabels=['no','yes'])\n",
    "    plt.title(f'Confusion Matrix - {model_name}')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for model_name in models.keys():\n",
    "    evaluate_model(model_name)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
