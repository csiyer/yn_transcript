{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL EVALUATION\n",
    "\n",
    "This file has been placed in `archive` because it was only used once. This script first parses the `example_transcripts` and saves all questions to `question_datasets/all_questions_unlabeled.csv`. \n",
    "\n",
    "It then selects a subset of questions (by embedding, clustering, and sampling from each cluster) to populate a training set for manual labeling. I manually labeled these 1000 questions, creating `labeled_training_questions.csv`. \n",
    "\n",
    "Finally, it evaluates the performance of a set of models from HuggingFace on the labeled training set, which I used to select the final model.\n",
    "\n",
    "This cannot be run on the `hcrc_env` because it uses some packages not included. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "ArgumentError: activate does not accept more than one argument:\n",
      "['lab', '#', 'using', 'a', 'different', 'environment', 'with', 'more', 'of', 'the', 'necessary', 'libraries']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%conda activate lab # using a different environment with more of the necessary libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create labeled question set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SAVE ALL QUESTIONS TO CSV\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# split this process into ranges of lines corresponding to each witness (to parallelize for speed)\n",
    "# def process_one_range(range_of_lines, lines):\n",
    "# initialize variables to be used as we loop\n",
    "current_witness = ''\n",
    "current_witness_side = ''\n",
    "current_examination = ''\n",
    "current_examiner = ''\n",
    "qdata = pd.DataFrame(columns=['row_i', 'question_text', 'answer_yes_no', 'gpt_yes_no'])\n",
    "\n",
    "queries_to_make = []\n",
    "\n",
    "for i in range(len(lines)):\n",
    "    line = lines[i]\n",
    "\n",
    "    if line_is_witness_identifier(lines, i):\n",
    "        current_witness = clean_simple_line(line)\n",
    "        current_witness_side = who_presents_this_witness(lines, i)\n",
    "\n",
    "    elif line_is_examination_identifier(lines, i):\n",
    "        current_examiner = ''\n",
    "        current_examination = clean_simple_line(line)\n",
    "\n",
    "    elif line_is_examiner_identifier(line):\n",
    "        current_examiner = clean_examiner_name(line)\n",
    "\n",
    "    elif is_answer(line):\n",
    "        # we may need to guess necessary preceding info if there was an error in pdf reading\n",
    "        if current_examiner == '': \n",
    "            current_examiner = guess_examiner(current_witness_side, current_examination) \n",
    "\n",
    "        active_question = guess_previous_question(lines, i, current_examiner) \n",
    "            \n",
    "        if '?' in active_question: # to rule out things like \"Q. Good morning.\"\n",
    "            yes_no_answer = is_yes_no_answer(lines, i, current_examiner)\n",
    "            # yes_no_question = True if yes_no_answer=='yes' else is_yes_no(clean_question(active_question))\n",
    "            if yes_no_answer!='yes': queries_to_make.append((i, clean_question(active_question)))\n",
    "\n",
    "            yes_no_answer_tag = 'Y' if yes_no_answer=='yes' else 'n'\n",
    "            yes_no_question_tag = '' # 'Y' if yes_no_question else 'n'\n",
    "\n",
    "            qdata.loc[len(qdata)] = [i, clean_question(active_question), yes_no_answer_tag, yes_no_question_tag]\n",
    "    elif active_question:\n",
    "        active_question += line # if we started a question, add this line. resets at every answer or special identifying line\n",
    "\n",
    "print('queries to make: ', len(queries_to_make))\n",
    "qdata.to_csv('question_datasets/all_questions_unlabeled.csv', index=False)\n",
    "\n",
    "# def run(arg):\n",
    "#     i, question = arg\n",
    "#     return i, is_yes_no(question)\n",
    "# with ThreadPoolExecutor(max_workers=128) as executor:\n",
    "#     results = list(executor.map(run, queries_to_make))\n",
    "\n",
    "# for i,gpt_out in results:\n",
    "#     out = 'Y' if gpt_out else 'n'\n",
    "#     qdata.loc[i, 'gpt_yes_no'] = out\n",
    "# qdata.to_csv('all_questions_unlabeled.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence embedding\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "df = pd.read_csv('/Users/ciyer/Documents/yn_transcript/all_questions_unlabeled.csv')\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(df['question_text'].tolist())\n",
    "normalized_embeddings = normalize(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "pca = PCA()\n",
    "pca_result = pca.fit_transform(embeddings)\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "# Plot the explained variance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio.cumsum(), 'bo-')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance Ratio')\n",
    "plt.title('Explained Variance by Principal Components')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50-means clustering and save output\n",
    "n_clusters = 50 # elbow-ish of the PCA curve\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "cluster_labels = kmeans.fit_predict(normalized_embeddings)\n",
    "\n",
    "df['cluster'] = cluster_labels\n",
    "\n",
    "# Function to sample from each cluster\n",
    "def sample_from_cluster(cluster_id, n_samples=20):\n",
    "    cluster_indices = df[df['cluster'] == cluster_id].index\n",
    "    if len(cluster_indices) <= n_samples:\n",
    "        return cluster_indices\n",
    "    else:\n",
    "        return np.random.choice(cluster_indices, n_samples, replace=False)\n",
    "\n",
    "# Sample 20 sentences from each cluster\n",
    "sampled_indices = []\n",
    "for i in range(n_clusters):\n",
    "    sampled_indices.extend(sample_from_cluster(i))\n",
    "\n",
    "# Convert to list if it's not already\n",
    "sampled_indices = list(sampled_indices)\n",
    "\n",
    "# Shuffle the sampled indices\n",
    "np.random.shuffle(sampled_indices)\n",
    "\n",
    "# Ensure we have exactly 1000 samples\n",
    "if len(sampled_indices) > 1000:\n",
    "    sampled_indices = sampled_indices[:1000]\n",
    "\n",
    "df.loc[sampled_indices,].drop('cluster', axis=1).to_csv('/Users/ciyer/Documents/yn_transcript/training_questions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model:  gpt\n",
      "Model: gpt_yes_no, Accuracy: 0.7016\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model:  facebook/bart-large-mnli\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 54\u001b[0m\n\u001b[0;32m     50\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_name \u001b[38;5;129;01min\u001b[39;00m models\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m---> 54\u001b[0m     \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m train_data\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabeled_training_set_model_predictions.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[1], line 39\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(model_name, train_data)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpt\u001b[39m\u001b[38;5;124m'\u001b[39m: model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpt_yes_no\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m train_data\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m---> 39\u001b[0m     \u001b[43madd_model_preds\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(train_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmanual_yes_no\u001b[39m\u001b[38;5;124m'\u001b[39m], train_data[model_name])\n\u001b[0;32m     42\u001b[0m cm \u001b[38;5;241m=\u001b[39m confusion_matrix(train_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmanual_yes_no\u001b[39m\u001b[38;5;124m'\u001b[39m], train_data[model_name], labels\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mno\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myes\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[1;32mIn[1], line 26\u001b[0m, in \u001b[0;36madd_model_preds\u001b[1;34m(model_name, train_data)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pipe \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzero-shot-classification\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     25\u001b[0m     candidate_labels \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myes/no question\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mother question\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m---> 26\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mclassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mquestion_text\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcandidate_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m     results \u001b[38;5;241m=\u001b[39m [r[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m results]\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m pipe \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext-classification\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\ciyer\\miniconda3\\envs\\lab\\Lib\\site-packages\\transformers\\pipelines\\zero_shot_classification.py:206\u001b[0m, in \u001b[0;36mZeroShotClassificationPipeline.__call__\u001b[1;34m(self, sequences, *args, **kwargs)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to understand extra arguments \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ciyer\\miniconda3\\envs\\lab\\Lib\\site-packages\\transformers\\pipelines\\base.py:1224\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m can_use_iterator:\n\u001b[0;32m   1221\u001b[0m     final_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   1222\u001b[0m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[0;32m   1223\u001b[0m     )\n\u001b[1;32m-> 1224\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfinal_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n\u001b[0;32m   1226\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ciyer\\miniconda3\\envs\\lab\\Lib\\site-packages\\transformers\\pipelines\\pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[1;32m--> 124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator)\n\u001b[0;32m    125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ciyer\\miniconda3\\envs\\lab\\Lib\\site-packages\\transformers\\pipelines\\pt_utils.py:269\u001b[0m, in \u001b[0;36mPipelinePackIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    266\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m accumulator\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_last:\n\u001b[1;32m--> 269\u001b[0m     processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    271\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed, torch\u001b[38;5;241m.\u001b[39mTensor):\n",
      "File \u001b[1;32mc:\\Users\\ciyer\\miniconda3\\envs\\lab\\Lib\\site-packages\\transformers\\pipelines\\base.py:1150\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[1;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[0;32m   1148\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[0;32m   1149\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m-> 1150\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1151\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m   1152\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ciyer\\miniconda3\\envs\\lab\\Lib\\site-packages\\transformers\\pipelines\\zero_shot_classification.py:229\u001b[0m, in \u001b[0;36mZeroShotClassificationPipeline._forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(model_forward)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    228\u001b[0m     model_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 229\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    231\u001b[0m model_outputs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcandidate_label\u001b[39m\u001b[38;5;124m\"\u001b[39m: candidate_label,\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msequence\u001b[39m\u001b[38;5;124m\"\u001b[39m: sequence,\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_last\u001b[39m\u001b[38;5;124m\"\u001b[39m: inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_last\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moutputs,\n\u001b[0;32m    236\u001b[0m }\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model_outputs\n",
      "File \u001b[1;32mc:\\Users\\ciyer\\miniconda3\\envs\\lab\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ciyer\\miniconda3\\envs\\lab\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ciyer\\miniconda3\\envs\\lab\\Lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:1902\u001b[0m, in \u001b[0;36mBartForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1897\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1898\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m   1899\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing input embeddings is currently not supported for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1900\u001b[0m     )\n\u001b[1;32m-> 1902\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1903\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1904\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1905\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1906\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1907\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1908\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1909\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1910\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1911\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1912\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1913\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1914\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1915\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1917\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1918\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# last hidden state\u001b[39;00m\n\u001b[0;32m   1920\u001b[0m eos_mask \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39meq(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39meos_token_id)\u001b[38;5;241m.\u001b[39mto(hidden_states\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\ciyer\\miniconda3\\envs\\lab\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ciyer\\miniconda3\\envs\\lab\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ciyer\\miniconda3\\envs\\lab\\Lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:1628\u001b[0m, in \u001b[0;36mBartModel.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1621\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m BaseModelOutput(\n\u001b[0;32m   1622\u001b[0m         last_hidden_state\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m   1623\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1624\u001b[0m         attentions\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1625\u001b[0m     )\n\u001b[0;32m   1627\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m-> 1628\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1629\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1630\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1631\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1632\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1633\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1634\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1635\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1636\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1637\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1638\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1639\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1640\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1641\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1643\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[0;32m   1644\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m decoder_outputs \u001b[38;5;241m+\u001b[39m encoder_outputs\n",
      "File \u001b[1;32mc:\\Users\\ciyer\\miniconda3\\envs\\lab\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ciyer\\miniconda3\\envs\\lab\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ciyer\\miniconda3\\envs\\lab\\Lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:1481\u001b[0m, in \u001b[0;36mBartDecoder.forward\u001b[1;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1468\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m   1469\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m   1470\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1478\u001b[0m         use_cache,\n\u001b[0;32m   1479\u001b[0m     )\n\u001b[0;32m   1480\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1481\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1485\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1486\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1488\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[0;32m   1489\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1491\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1492\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1493\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1494\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\ciyer\\miniconda3\\envs\\lab\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ciyer\\miniconda3\\envs\\lab\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ciyer\\miniconda3\\envs\\lab\\Lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:805\u001b[0m, in \u001b[0;36mBartDecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[0;32m    803\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_fn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(hidden_states))\n\u001b[0;32m    804\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(hidden_states, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_dropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[1;32m--> 805\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    806\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(hidden_states, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[0;32m    807\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[1;32mc:\\Users\\ciyer\\miniconda3\\envs\\lab\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ciyer\\miniconda3\\envs\\lab\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ciyer\\miniconda3\\envs\\lab\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('/Users/ciyer/Documents/yn_transcript/question_datasets/labeled_training_questions.csv')\n",
    "train_data = df[df.answer_yes_no == 'no'].copy()\n",
    "\n",
    "models = {\n",
    "    'gpt': {'pipeline': ''},\n",
    "    \"facebook/bart-large-mnli\": {'yes/no question': 'yes', 'other question': 'no', 'pipeline': 'zero-shot-classification'},\n",
    "    \"scott-routledge/bert-question-classifier\": {'LABEL_0': 'yes', 'LABEL_1': 'no', 'LABEL_2': 'yes', 'pipeline': 'text-classification'},\n",
    "    \"sophiaqho/question_classifier_model_v2\": {'LABEL_0': 'yes', 'LABEL_1': 'no', 'pipeline': 'text-classification'},\n",
    "    \"alangpp255/Question_classifier_V2\": {'TF': 'yes', 'WH': 'no', 'pipeline': 'text-classification'},\n",
    "    \"ndavid/binary-question-classifier-bert\": {'LABEL_0': 'no', 'LABEL_1': 'yes', 'pipeline': 'text-classification'},\n",
    "    \"PrimeQA/tydi-boolean_question_classifier-xlmr_large-20221117\": {'LABEL_0': 'yes', 'LABEL_1': 'no', 'pipeline': 'text-classification'},\n",
    "    \"PrimeQA/tydiqa-boolean-question-classifier\": {'LABEL_0': 'no', 'LABEL_1': 'yes', 'pipeline': 'text-classification'}\n",
    "}\n",
    "\n",
    "def add_model_preds(model_name, train_data):\n",
    "    pipe = models[model_name]['pipeline']\n",
    "    classifier = pipeline(pipe, model=model_name)\n",
    "    if pipe == 'zero-shot-classification':\n",
    "        candidate_labels = [\"yes/no question\", \"other question\"]\n",
    "        results = classifier(train_data['question_text'].tolist(), candidate_labels)\n",
    "        results = [r['labels'][0] for r in results]\n",
    "    elif pipe == 'text-classification':\n",
    "        results = [classifier(q)[0]['label'] for q in train_data.question_text]\n",
    "\n",
    "    predicted_labels = [models[model_name][p] for p in results]\n",
    "    train_data[model_name] = predicted_labels\n",
    "\n",
    "def evaluate_model(model_name, train_data):\n",
    "    print('Starting model: ', model_name)\n",
    "    \n",
    "    if model_name == 'gpt': model_name = 'gpt_yes_no'\n",
    "    if model_name not in train_data.columns:\n",
    "        add_model_preds(model_name, train_data)\n",
    "\n",
    "    accuracy = accuracy_score(train_data['manual_yes_no'], train_data[model_name])\n",
    "    cm = confusion_matrix(train_data['manual_yes_no'], train_data[model_name], labels=['no','yes'])\n",
    "\n",
    "    print(f\"Model: {model_name}, Accuracy: {accuracy:.4f}\")\n",
    "    plt.figure(figsize=(4,3))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['no','yes'], yticklabels=['no','yes'])\n",
    "    plt.title(f'Confusion Matrix - {model_name}')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "for model_name in models.keys():\n",
    "    evaluate_model(model_name, train_data)\n",
    "\n",
    "train_data.to_csv('labeled_training_set_model_predictions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
